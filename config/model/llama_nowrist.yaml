name: 'llama_dp'
vocab_size: 768
num_layers: 12
dropout_rate: 0.0
time_sequence_length: 5
include_prev_timesteps_actions: False
freeze_backbone: False
use_qformer: True
use_wrist_img: False
use_depth_img: false
input_size: (182, 322)